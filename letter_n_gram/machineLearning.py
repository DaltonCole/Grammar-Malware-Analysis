import argparse
import os
import io
import sys
from threading import Thread, Lock
import json
import queue
from copy import deepcopy
import numpy as np
from sklearn.model_selection import KFold
from sklearn.metrics import confusion_matrix
from statistics import mean
import progressbar
from math import factorial

class MachineLearning:
	def __init__(self, n_grams=4, show_progress_bar=True, save_file='./results.txt'):
		self.show_progress_bar = show_progress_bar
		self.feature_selection = False
		self.save_file = save_file
		self.n = n_grams

		self.feature_order = []
		self.feature_set = np.array(0)
		self.labels = np.array(0)

		self.benign_count = 0
		self.malicious_count = 0

		self.splits = 10 # k-split cross validation

		self.mutex = Lock()
		self.max_thread_count = 32
		self.queue = queue.Queue()

	def setup(self, benign_directory, malicious_directory, rule_file='./all_characters.json', ):
		'''
		'''
		self.n_gram_setup(rule_file)
		self.feature_vector_setup(benign_directory, malicious_directory)

	def feature_selection_do(self):
		self.feature_selection = True

		# Perform feature selection
		from sklearn.feature_selection import VarianceThreshold

		sel = VarianceThreshold(threshold=(.8 * (1 - .8)))
		self.feature_set = sel.fit_transform(self.feature_set)


	def n_gram_setup(self, rule_file='./all_characters.json'):
		# List of rules
		rules = None

		# Open rules file
		with open(rule_file, 'r') as f:
			rules = json.load(f)

		self.rule_mapping = {}
		for index, rule in enumerate(rules):
			self.rule_mapping[rule] = index
  
		self.number_of_features = len(rules) ** self.n
		self.number_of_rules = len(rules)

	def _rule_to_feature_vector_index(self, rule_indexes: list):
		'''
		'''
		index = 0

		for i, rule_num in enumerate(rule_indexes[::-1]):
			index += ((self.number_of_rules ** i) * rule_num)

		return index

	def feature_vector_setup(self, benign_directory: str, malicious_directory: str):
		"""Finds the feature dictionaries of each file in the benign and 
		malicious directories with the given file extensions

		Args:
			benign_directory (str): Directory containing the benign files
			malicious_directory (str): Directory containing the malicious files

		Returns:
			Nothing. Populates class instance variables
				self.feature_set: [[]] List of list of features
				self.labels = [] Labels for each list of features in feature_set
		"""
		# Get benign code file paths
		benign_file_paths = []
		for root, dirs, files in os.walk(benign_directory):
			for file in files:
				benign_file_paths.append(os.path.join(root, file))

		# Get malicious code file paths
		malicious_file_paths = []
		for root, dirs, files in os.walk(malicious_directory):
			for file in files:
				malicious_file_paths.append(os.path.join(root, file))

		self.feature_set = np.empty((len(benign_file_paths) + len(malicious_file_paths),
			self.number_of_features), bool)

		with progressbar.ProgressBar(max_value=len(benign_file_paths) + len(malicious_file_paths)) as bar:
			self.count = 0
			if self.show_progress_bar:
				bar.update(0)

			threads = []
			for i in range(self.max_thread_count):
				t = Thread(target=self._feature_set_mapper_queue)
				t.start()
				threads.append(t)

			# For each file
			for row, file_path in enumerate(benign_file_paths + malicious_file_paths):
				item = {'file': file_path, 'row': row, 'bar': bar}
				# Add to threading queue
				self.queue.put(item)

			# Wait until all tasks are finished
			self.queue.join()

			# Stop threads
			for i in range(self.max_thread_count):
				self.queue.put(None)
			for t in threads:
				t.join()

		# Add labels (benign = 0, malicious = 1)
		self.labels = np.zeros(len(benign_file_paths) + len(malicious_file_paths))
		for i in range(len(benign_file_paths), self.labels.size):
			self.labels[i] = 1
		###################################

	def _feature_set_mapper_queue(self):
		while True:
			arguments = self.queue.get()
			if arguments is None:
				break

			file_path = arguments['file']
			row = arguments['row']
			bar = arguments['bar']
			data = []

			with open(file_path, 'rb', buffering=0) as file:
				try:
					for line in file:
						try:
							for char in line:
								char = ord(chr(char).lower())
								data.append(char)
						except Exception as e:
							print(e)

				except Exception as e:
					print(e)

			# Add numerical form of data
			data_num = [self.rule_mapping[x] for x in data]

			feature_vector = self._feature_set_mapper(data_num)

			self.mutex.acquire()
			try:
				# Add feature vector to feature matrix
				for col, feature in enumerate(feature_vector):
					self.feature_set[row, col] = feature

				self.count += 1
				if self.show_progress_bar:
					bar.update(self.count)
			finally:
				self.mutex.release()

			self.queue.task_done()

	def _feature_set_mapper(self, data: list):
		'''
		self.n_gram_setup should be called first
		'''
		# Fill feature vector with zeros
		feature_vector = np.zeros(self.number_of_features, dtype=bool)

		# For each n-gram
		for index in range(len(data) - self.n + 1):
			# Get n-gram features
			n_gram = []
			for i in range(self.n):
				n_gram.append(data[index + i])
			# Convert to hash-able type
			n_gram = tuple(n_gram)

			# Find n-gram position in feature vector and set bit
			feature_vector[self._rule_to_feature_vector_index(n_gram)] = True

		return feature_vector


	def pickle_it(self, directory, save_file):
		import pickle
		if not os.path.exists(directory):
			os.makedirs(directory)

		path = directory + '/' + save_file + '__'

		pickle.dump(self.feature_set, open(path + "feature_set.p", "wb"))
		pickle.dump(self.labels, open(path + "feature_labels.p", "wb"))
		print("Pickled Success!")

	def unpickle_it(self, directory):
		import pickle
		self.feature_set = pickle.load(open(directory + "/feature_set.p", "rb"))
		self.labels = pickle.load(open(directory + "/feature_labels.p", "rb"))

	def _cross_val_score_queue(self):
		while True:
			arguments = self.queue.get()
			if arguments is None:
				break
			clf = deepcopy(arguments['clf'])
			train_index = arguments['train_index']
			test_index = arguments['test_index']

			x_train, x_test = self.feature_set[train_index], self.feature_set[test_index]
			y_train, y_test = self.labels[train_index], self.labels[test_index]

			clf.fit(x_train, y_train)
			prediction = clf.predict(x_test)

			tn, fp, fn, tp = confusion_matrix(y_test, prediction).ravel()

			self.mutex.acquire()
			try:
				self.tn_lst.append(tn)
				self.fp_lst.append(fp)
				self.fn_lst.append(fn)
				self.tp_lst.append(tp)
			finally:
				self.mutex.release()

			self.queue.task_done()

	def _cross_val_score(self, clf, algorithm):
		kf = KFold(n_splits=self.splits, shuffle=True)

		self.tn_lst = []
		self.fp_lst = []
		self.fn_lst = []
		self.tp_lst = []

		self.mutex = Lock()
		self.queue = queue.Queue()

		threads = []
		for i in range(self.max_thread_count):
			t = Thread(target=self._cross_val_score_queue)
			t.start()
			threads.append(t)

		for train_index, test_index in kf.split(self.feature_set):
			item = {'clf': clf, 'train_index': train_index, 'test_index': test_index}
			self.queue.put(item)

		# Wait until all tasks are finished
		self.queue.join()

		# Stop threads
		for i in range(self.max_thread_count):
			self.queue.put(None)
		for t in threads:
			t.join()

		tn = mean(self.tn_lst)
		fp = mean(self.fp_lst)
		fn = mean(self.fn_lst)
		tp = mean(self.tp_lst)

		recall = tp / (tp + fn)
		precision =  tp / (tp + fp)
		f1_score = 2 * (precision * recall) / (precision + recall)

		with open(self.save_file, 'a') as f:
			f.write('\n')
			f.write('Algorithm: {}\n'.format(algorithm))
			f.write('{}-gram\n'.format(self.n))
			if self.feature_selection == True:
				f.write('Feature Selection Applied\n')
			f.write('K-Folds: {}\n'.format(self.splits))
			f.write('Total Samples: {}\n'.format(self.labels.size))
			f.write('Total Malicious Samples: {}\n'.format(np.count_nonzero(self.labels == 1)))
			f.write('Total Benign Samples: {}\n'.format(np.count_nonzero(self.labels == 0)))
			f.write('Total Features: {}\n'.format(np.size(self.feature_set, 1)))
			f.write('True Positive: {}\n'.format(tp))
			f.write('False Positive: {}\n'.format(fp))
			f.write('True Negative: {}\n'.format(tn))
			f.write('False Negative: {}\n'.format(fn))
			f.write('Recall: {}\n'.format(recall))
			f.write('Precision: {}\n'.format(precision))
			f.write('F1 Score: {}\n'.format(f1_score))
			f.write('\n')


	def svm(self):
		from sklearn import svm

		clf = svm.SVC()
		self._cross_val_score(clf, 'SVM')

	def naive_bayes(self):
		from sklearn.naive_bayes import GaussianNB

		gnd = GaussianNB()
		self._cross_val_score(gnd, 'Naive Bayes')

	def decision_tree(self):
		from sklearn import tree

		clf = tree.DecisionTreeClassifier()
		self._cross_val_score(clf, 'Decision Tree')

	def random_forest(self):
		from sklearn.ensemble import RandomForestClassifier

		clf = RandomForestClassifier(n_estimators=100)
		self._cross_val_score(clf, 'Random Forest')

	def neural_network(self):
		from sklearn.neural_network import MLPClassifier

		# For small datasets, however, ‘lbfgs’ can converge faster and perform better.
		clf = MLPClassifier(solver='lbfgs', alpha=1e-5, \
							hidden_layer_sizes=(5,2), random_state=1)
		self._cross_val_score(clf, 'Neural Network')

	def stochastic_gradient_descent(self):
		from sklearn.linear_model import SGDClassifier
		clf = SGDClassifier(max_iter=1000, tol=1e-3)
		self._cross_val_score(clf, 'Stochastic Gradient Descent Classifier')

	def knn(self):
		from sklearn.neighbors import KNeighborsClassifier
		clf = KNeighborsClassifier(weights='distance')
		self._cross_val_score(clf, 'K-Nearest Neighbors')


def ml(show_pb=False, pickle_dir='./pickle'):
	save_file = './results/results_full_features.txt'
	if not os.path.exists('./results/'):
		os.makedirs('./results/')
	with open(save_file, 'a') as f:
		f.write('\n-------------------------------------------------------------------------\n')

	benign_dir = '/home/drc/Desktop/Research/javascript_web_crawler/scripts'
	malicious_dir = '/home/drc/Desktop/javascript-malware-collection'

	for n in range(1, 3):
		# Non-Feature Reduction
		ml = MachineLearning(n_grams=n, show_progress_bar=show_pb, save_file=save_file)
		ml.setup(benign_directory=benign_dir, malicious_directory=malicious_dir)

		#ml.setup(benign_directory='/home/drc/Desktop/javascript-malware-collection/1936', 
		#	malicious_directory='/home/drc/Desktop/javascript-malware-collection/1936')


		ml.pickle_it(pickle_dir, 'ml__n_{}__feature_selection_{}'.format(n, False))
	
		ml.decision_tree()
		#'''
		ml.random_forest()
		ml.neural_network()
		ml.naive_bayes()
		ml.stochastic_gradient_descent()
		ml.knn()
		ml.svm()
		#'''


		# Feature Reduction
		ml.feature_selection_do()
		ml.pickle_it(pickle_dir, 'ml__n_{}__feature_selection_{}'.format(n, True))
	
		ml.decision_tree()
		#'''
		ml.random_forest()
		ml.neural_network()
		ml.naive_bayes()
		ml.stochastic_gradient_descent()
		ml.knn()
		ml.svm()
		#'''

def main():
	show_progress_bar = True
	pickle_dir = './pickle'
	ml(show_progress_bar)
			
	'''
	ml.feature_vector_setup('/home/drc/Desktop/Research/grammar_malware_analysis/antlr/malicious_samples/1',
		'/home/drc/Desktop/Research/grammar_malware_analysis/antlr/malicious_samples/1',
		'rule_order.json')
	'''

	'''
	import argparse

	parser = argparse.ArgumentParser()
	parser.add_argument("parser", help="Path to parser to use", type=str)
	parser.add_argument("benign_directory", help="Directory containing the benign files", type=str)
	parser.add_argument("malicious_directory", help="Directory containing the malicious files", type=str)
	parser.add_argument("file_extensions", help="Comma delimited list of acceptable file extensions", type=str)
	args = parser.parse_args()

	extensions = []
	for ext in args.file_extensions.split(','):
		extensions.append('.{}'.format(ext))
	args.file_extensions = extensions

	ml = machineLearning(args.parser)
	
	ml.feature_vector_setup(args.benign_directory, args.malicious_directory, args.file_extensions);
	#print(ml.feature_set)
	ml.pickle_it()
	#ml.unpickle_it()
	'''

	'''
	ml.svm()
	ml.naive_bayes()
	ml.decision_tree()
	ml.random_forest()
	ml.neural_network()
	'''


if __name__ == '__main__':
    main()