%% bare_conf.tex
%% V1.4
%% 8/27/2019
%% by Dalton Cole
%%
\documentclass[conference]{IEEEtran}


% *** CITATION PACKAGES ***
\usepackage{cite}


% *** GRAPHICS RELATED PACKAGES ***
\usepackage[pdftex]{graphicx}
% declare the path(s) where your graphic files are
% \graphicspath{{../pdf/}{../jpeg/}}
% and their extensions so you won't have to specify these with
% every instance of \includegraphics
% \DeclareGraphicsExtensions{.pdf,.jpeg,.png}


% *** MATH PACKAGES ***
\usepackage[cmex10]{amsmath}

% *** SPECIALIZED LIST PACKAGES ***
\usepackage{algorithmic}

% *** ALIGNMENT PACKAGES ***
\usepackage{array}
% Frank Mittelbach's and David Carlisle's array.sty patches and improves
% the standard LaTeX2e array and tabular environments to provide better
% appearance and additional user controls. As the default LaTeX2e table
% generation code is lacking to the point of almost being broken with
% respect to the quality of the end results, all users are strongly
% advised to use an enhanced (at the very least that provided by array.sty)
% set of table tools. array.sty is already installed on most systems. The
% latest version and documentation can be obtained at:
% http://www.ctan.org/tex-archive/macros/latex/required/tools/

% *** SUBFIGURE PACKAGES ***
\ifCLASSOPTIONcompsoc
  \usepackage[caption=false,font=normalsize,labelfont=sf,textfont=sf]{subfig}
\else
  \usepackage[caption=false,font=footnotesize]{subfig}
\fi

% *** FLOAT PACKAGES ***
\usepackage{fixltx2e}

% *** PDF, URL AND HYPERLINK PACKAGES ***
\usepackage{url}
% Basically, \url{my_url_here}.

% correct bad hyphenation here
\hyphenation{op-tical net-works semi-conduc-tor}


%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\begin{document}
%
% paper title
% can use linebreaks \\ within to get better formatting as desired
% Do not put math or special symbols in the title.
\title{TODO}


% author names and affiliations
% use a multiple column layout for up to three different
% affiliations
\author{
\IEEEauthorblockN{Dalton Cole}
\IEEEauthorblockA{Department of Computer Science\\
Missouri University of Science and Technology\\
Rolla, Missouri 65409-0350, USA\\
Email: drcgy5@mst.edu}
\and
\IEEEauthorblockN{Bruce McMillin}
\IEEEauthorblockA{Department of Computer Science\\
Missouri University of Science and Technology\\
Rolla, Missouri 65409-0350, USA\\
Email: ff@mst.edu}}

% make the title area
\maketitle

% As a general rule, do not put math, special symbols or citations
% in the abstract
\begin{abstract}
The abstract goes here.
\end{abstract}

% no keywords

% For peer review papers, you can put extra information on the cover
% page as needed:
% \ifCLASSOPTIONpeerreview
% \begin{center} \bfseries EDICS Category: 3-BBND \end{center}
% \fi
%
% For peerreview papers, this IEEEtran command inserts a page break and
% creates the second title. It will be ignored for other modes.
\IEEEpeerreviewmaketitle



\section{Introduction}
% no \IEEEPARstart
Intro

%\hfill mds 
%\hfill December 27, 2012

% Sub and subsub section EXAMPLE
%\subsection{Subsection Heading Here}
%Subsection text here.

%\subsubsection{Subsubsection Heading Here}
%Subsubsection text here.

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\section{Methodology}

An overview of the system is outlined in Figure \ref{fig:system_overview}. The following subsections outline in detail how each partition of the system functions. For how data was collected, see Section \ref{Experimental Evaluation}.

\input{figures/system_overview.tex}


% Overview - Mention overview table

\subsection{Antlr}

Antlr was used to parse the grammar \cite{antlr}. Antler is a framework designed to automatically generate code in a number of languages. The resulting supplementary files can be used to tokenize and parse given inputs. Antlr requires tokens and grammatical rules to be defined for the desired language in a g4 file. Antlr is designed for context-free grammars (CFG), however, by adding additional logic that is programming language specific, Antlr can handle context-sensitive languages.

Antlr parses samples using a LL(*) parser, which is a top-down approach. Top-down approaches work by starting at the top level of a parse tree and matches as it traverses down the tree. LL(*) performs leftmost deviation as it reads in tokens from left to right. The amount of tokens read in at a time to determine which grammar rule to use is determined by the * value. If $* = 1$, then only one token is required at a time to know which grammar rule to expand next.


\subsection{N-Gram}

A list of grammatical rules in order of which they were parsed is extracted from a given sample using Antlr. These rules were then grouped in over-lapping sequences of n rules where each sequence differed by its neighboring sequences by one rule. These sequences are called n-grams. A snippet of a n-gram extracted from our system can be found in Figure \ref{fig:n-gram-example}. Due to memory limitations, only sequences where $n = \{1, 2, 3\}$ were explored. To generate a feature vector of set length, every possible n-gram was composed.

\input{figures/n_gram_example.tex}


% Explain why a different number of features were used
% For memory and accuracy testing, we used a different
%   number of features for different tests

\subsection{Feature Reduction} \label{Methodology - Feature Reduction}

Due to the large number of features that n-gram produces, feature reduction is required to reduce the memory usage to a usable level. In order to read in saved data, incremental principal component analysis (IPCA) was selected. IPCA is able to read in information from disk in batches. From each read, IPCA finds orthogonal eigen vectors. After all rows of the dataframe have been read in, the top n eigen vectors with the most variance are selected to represent the features of the data. When the number of features are less than n, feature reduction is not applied. %This occurs in the trimmed and extra trimmed 1-gram datasets.

\subsection{Machine Learning}

A number of different machine learning methods were employed. These methods were chosen so the results could be compared to TODO LIST REFERENCES. K-Fold cross validation was used to train and test each machine learning classifier.

\subsubsection{Decision Trees}

entropy

\subsubsection{Random Forests}

\subsubsection{Neural Network}

\subsubsection{Naive Bayes}

\subsubsection{Gradient Decent}

\subsubsection{K-Nearest-Neighbors}

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\section{Experimental Evaluation} \label{Experimental Evaluation}

% Mention that JavaScript was used and why

\subsection{Benign Data Collection}

The benign data set consisted of JavaScript samples scraped from the top 1000 Alexa websites \cite{alexa}. Due to these site's popularity, it was assumed that each javascript sample scrapped from these sites were benign.

The python library scrapy was used to create a web-crawler and scrapper \cite{scrapy}. A set of starting URLs and acceptable domains were given to scrapy that consisted of the top 1000 Alexa websites. From here, any javascript that was enclosed in HTML script tags were scrapped from the site and saved into a hash-table. A hash-table was used to guarantee unique samples. A total of 201,901 scripts were scrapped using this method. Due to our use of Antlr, malformed JavaScript samples could not be parsed. The main malformation that was not accepted was the lack of use of semicolons. If a script did not use semicolons to end statements, the script was considered malformed and discarded. A full list of what is considered malformed can be found at \cite{javascript_grammar}. After malformed samples were removed from the set of possible scripts, 140,437 scripts remained. These scripts were feed into Antlr using the JavaScript grammar defined in \cite{javascript_grammar}. From Antlr the order in which the grammatical rules were parsed in were returned.

\subsection{Malicious Data Collection}

Malicious data consisted of known malicious JavaScript samples obtained from \cite{malicious_javascript_repo}. The majority of this malware was collected between 2015 and 2017. A random sample was ran through VirusTotal in order to verify that leading anti-virus scanners recognized the samples as malicious \cite{virustotal}. All samples that were randomly selected was marked malicious by VirusTotal. A total of 39,471 JavaScript samples were obtained. Similarly to the benign samples collected, some scripts were malformed. After removing the malformed samples, 15,096 samples remained. Using Antlr, the order in which the grammatical rules were parsed in were returned.

\subsection{Feature Reduction}

% Explain why feature Reduction was/was not used
Due to the large number of features that n-gram produces, three different feature sets were used. An unabridged dataset containing 112 features. This unabridged dataset contains every grammatical non-terminal in the original JavaScript grammar used to parse the language. From this unabridged dataset, two additional datasets were generated. A trimmed dataset containing 62 rules were generated by combining every Expression grammatical rule into a single grammatical rule. By combining similar rules into a single rule, it is hoped that not much information is lost. A third extra trimmed dataset was constructed by further combining similar rules until only 40 features remained.

The reason this feature reduction was required is due to n-gram's exponential nature. When using 1-gram with the original dataset, only 112 features are needed. However, when using 2-gram, 12544 features are required. With this exponential growth, more memory is required to store those features. In addition, a very sparse matrix will be produced since most of the n-gram rules will not be seen in the source program.

In addition to the above feature reduction methods, incremental principal component analysis (IPCA) was employed to further reduce the features. A brief overview of how IPCA functions is reviewed in Section \ref{Methodology - Feature Reduction}. One hundred orthogonal vectors with the most variance was selected. One hundred features were selected because it gave a good compromise to the number of features in the 1-gram unabridged dataset. With the number of samples explored, 100 features will easily fit into memory without the need to save to disk, thus training times were greatly reduced.

\subsection{Classifier Training}

% Used equal number of benign and malicious samples
For classification training, an equal number of benign and malicious samples were selected from each dataset. Given the size of the malicious dataset, 15,000 samples from each classification was used to train the models. Choosing an equal number of samples in each class was decided upon due to two reasons. 

Firstly, the imbalanced class issues for the majority of machine learning methods used in this paper have been explored in \cite{sun2009classification}. With regards to decision trees, Sun et al. described how during the pruning phase of generating a decision tree, the leaf nodes representing the minority class have a good chance of being pruned since they have little effect on the overall accuracy of that branch. In addition, with respect to neural networks, they found that the majority class in skewed data tends to dominate the gradient vector. The minority class may even need to traverse in an uphill direction to find the optimal values. 

Secondly, accuracy and precision are positively effected by skewed data. Generally, classifiers are able to classify the majority class correctly a larger percentage of the time. Strictly looking at accuracy, classifier accuracy will be inflated due to classifying the majority class correctly more often. This diminishes the actual results of the minority class that we are concerned about \cite{jeni2013facing}.

% Site paper saying skewed data increases accuracy


% over sampling. Reference a couple papers showing that undersampling is better than over sampling
% recall and precision are balanced


\subsection{Results}

Recall, Precision, and $F_1$ score are the three primary metrics used to compare each result. Formulas used to calculate these can be found in Equation \ref{equ:recall}, Equation \ref{equ:precision}, and Equation \ref{equ:f1_score}, respectively.

\begin{equation}
	Recall = \frac{True Positive}{True Positive + False Negative}
	\label{equ:recall}
\end{equation}

\begin{equation}
	Precision = \frac{True Positive}{True Positive + False Positive}
	\label{equ:precision}
\end{equation}

\begin{equation}
	F_1 = 2 * \frac{Precision * Recall}{Precision + Recall}
	\label{equ:f1_score}
\end{equation}

\subsubsection{Full Feature Set}

The results using the full unabridged original feature set can be found in Table \ref{table:full}. Only n-grams where $n = {1, 2}$ are explored due to memory limitations. The total number of features are 112 and 12544 for each respective n-gram. In general, each metric improved when more features were used. Decision tree, random forests, neural network, and the stochastic gradient decent classifier perform nearly equally and better than other algorithms with regards to their $F_1$ score.

\subsubsection{Full Feature Set with Feature Reduction}

The full unabridged original feature set with feature reduction can be found in Table \ref{table:full_feature_selectiond}. The 112 and 12544 features in the full feature set where $n = {1, 2}$ is reduced to 100 features. This is done through incremental principal component analysis (IPCA) which is described in Section \ref{Methodology - Feature Reduction}. Desicion tree, random forest, and stochastic gradient decent classifier performed better than other algorithms tested.

\subsubsection{Trimmed Feature Set}


% Lots of tables
\input{tables/full_table.tex}
\input{tables/full_feature_selectiond_table.tex}
\input{tables/trimmed_table.tex}
\input{tables/trimmed_feature_selected_table.tex}
\input{tables/super_trimmed_table.tex}
\input{tables/super_trimmed_feature_selected_table.tex}



%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\section{Discussion}

% This was in Reults, moved to here. Need to make sure contextually, it makes sense
When $n = 1$, the feature vector only consists of boolean values of whether or not the grammatical rule was used to parse a sample or not. When $n > 1$, how grammatical rules interact with one another starts to be explored. The improved results show that the way malware is structured is different than benign samples. This is further highlighted in different feature sets.

When feature reduction was used, results were similar to the non reduced feature set.

% Metntion how a higher percentage of malformed malicious scripts were encountered when compared to the number of malformed bening scripts


%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\section{Related Work}


% An example of a floating figure using the graphicx package.
% Note that \label must occur AFTER (or within) \caption.
% For figures, \caption should occur after the \includegraphics.
% Note that IEEEtran v1.7 and later has special internal code that
% is designed to preserve the operation of \label within \caption
% even when the captionsoff option is in effect. However, because
% of issues like this, it may be the safest practice to put all your
% \label just after \caption rather than within \caption{}.
%
% Reminder: the "draftcls" or "draftclsnofoot", not "draft", class
% option should be used if it is desired that the figures are to be
% displayed while in draft mode.
%
%\begin{figure}[!t]
%\centering
%\includegraphics[width=2.5in]{myfigure}
% where an .eps filename suffix will be assumed under latex, 
% and a .pdf suffix will be assumed for pdflatex; or what has been declared
% via \DeclareGraphicsExtensions.
%\caption{Simulation Results.}
%\label{fig_sim}
%\end{figure}

% Note that IEEE typically puts floats only at the top, even when this
% results in a large percentage of a column being occupied by floats.


% An example of a double column floating figure using two subfigures.
% (The subfig.sty package must be loaded for this to work.)
% The subfigure \label commands are set within each subfloat command,
% and the \label for the overall figure must come after \caption.
% \hfil is used as a separator to get equal spacing.
% Watch out that the combined width of all the subfigures on a 
% line do not exceed the text width or a line break will occur.
%
%\begin{figure*}[!t]
%\centering
%\subfloat[Case I]{\includegraphics[width=2.5in]{box}%
%\label{fig_first_case}}
%\hfil
%\subfloat[Case II]{\includegraphics[width=2.5in]{box}%
%\label{fig_second_case}}
%\caption{Simulation results.}
%\label{fig_sim}
%\end{figure*}
%
% Note that often IEEE papers with subfigures do not employ subfigure
% captions (using the optional argument to \subfloat[]), but instead will
% reference/describe all of them (a), (b), etc., within the main caption.


% An example of a floating table. Note that, for IEEE style tables, the 
% \caption command should come BEFORE the table. Table text will default to
% \footnotesize as IEEE normally uses this smaller font for tables.
% The \label must come after \caption as always.
%
%\begin{table}[!t]
%% increase table row spacing, adjust to taste
%\renewcommand{\arraystretch}{1.3}
% if using array.sty, it might be a good idea to tweak the value of
% \extrarowheight as needed to properly center the text within the cells
%\caption{An Example of a Table}
%\label{table_example}
%\centering
%% Some packages, such as MDW tools, offer better commands for making tables
%% than the plain LaTeX2e tabular which is used here.
%\begin{tabular}{|c||c|}
%\hline
%One & Two\\
%\hline
%Three & Four\\
%\hline
%\end{tabular}
%\end{table}


% Note that IEEE does not put floats in the very first column - or typically
% anywhere on the first page for that matter. Also, in-text middle ("here")
% positioning is not used. Most IEEE journals/conferences use top floats
% exclusively. Note that, LaTeX2e, unlike IEEE journals/conferences, places
% footnotes above bottom floats. This can be corrected via the \fnbelowfloat
% command of the stfloats package.



\section{Conclusion}
The conclusion goes here.




% conference papers do not normally have an appendix


% use section* for acknowledgement
\section*{Acknowledgment}


The authors would like to thank...


% trigger a \newpage just before the given reference
% number - used to balance the columns on the last page
% adjust value as needed - may need to be readjusted if
% the document is modified later
%\IEEEtriggeratref{8}
% The "triggered" command can be changed if desired:
%\IEEEtriggercmd{\enlargethispage{-5in}}

% references section

% can use a bibliography generated by BibTeX as a .bbl file
% BibTeX documentation can be easily obtained at:
% http://www.ctan.org/tex-archive/biblio/bibtex/contrib/doc/
% The IEEEtran BibTeX style support page is at:
% http://www.michaelshell.org/tex/ieeetran/bibtex/
%\bibliographystyle{IEEEtran}
% argument is your BibTeX string definitions and bibliography database(s)
%\bibliography{IEEEabrv,../bib/paper}
%
% <OR> manually copy in the resultant .bbl file
% set second argument of \begin to the number of references
% (used to reserve space for the reference number labels box)
\begin{thebibliography}{1}

\bibitem{zozzle}
Charlie Curtsinger, Benjamin Livshits, Benjamin~G Zorn, and Christian Seifert.
\newblock Zozzle: Fast and precise in-browser javascript malware detection.
\newblock In {\em USENIX Security Symposium}, pages 33--48. San Francisco,
  2011.

\bibitem{antlr}
Antlr.
\newblock \url{https://www.antlr.org/}.
\newblock Accessed: 2019-08-27.

\end{thebibliography}






% that's all folks
\end{document}


