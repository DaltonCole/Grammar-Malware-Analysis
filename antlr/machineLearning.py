import argparse
import os
import io
import sys
from threading import Thread, Lock
import json
import queue
from copy import deepcopy
import numpy as np
from sklearn.model_selection import KFold
from sklearn.metrics import confusion_matrix
from statistics import mean
import progressbar
from math import factorial

"""
# NOTE ************************************************
# If number of times each grammar rule isn't useful, try percentage
# of how often the rule is used in the file (ie, normalize the data)

* Use tokens as a rule set as well
########################################################
"""

class MachineLearning:
	def __init__(self, n_grams=4, show_progress_bar=True, save_file='./results.txt', trimmed_rules={}):
		self.show_progress_bar = show_progress_bar
		self.feature_selection = False
		self.save_file = save_file
		self.n = n_grams

		self.feature_order = []
		self.feature_set = np.array(0)
		self.labels = np.array(0)

		self.benign_count = 0
		self.malicious_count = 0

		self.splits = 10 # k-split cross validation

		self.mutex = Lock()
		self.max_thread_count = 32
		self.queue = queue.Queue()

		self.trimmed_rules = trimmed_rules

	def setup(self, benign_directory, malicious_directory, file_name='rule_order.json', 
		rule_file='./JavaScript/js_possible_rules.txt', ):
		'''
		'''
		self.n_gram_setup(rule_file)
		self.feature_vector_setup(benign_directory, malicious_directory, file_name)

	def feature_selection_do(self):
		self.feature_selection = True

		# Perform feature selection
		from sklearn.feature_selection import VarianceThreshold

		sel = VarianceThreshold(threshold=(.8 * (1 - .8)))
		self.feature_set = sel.fit_transform(self.feature_set)


	def n_gram_setup(self, rule_file='./JavaScript/js_possible_rules.txt'):
		# List of rules
		rules = set()
		self.bad_rules = set()
		# Open rules file
		with open(rule_file, 'r') as f:
			for line in f:
				# Remove rules commented out with '#'
				if not line.startswith('#'):
					line = line.strip().lower()

					if line in self.trimmed_rules:
						line = self.trimmed_rules[line]

					rules.add(line)
				else:
					self.bad_rules.add((line[1:]).strip().lower())

		self.rule_mapping = {}
		for index, rule in enumerate(rules):
			self.rule_mapping[rule] = index

		self.number_of_features = len(rules) ** self.n
		self.number_of_rules = len(rules)

	def _rule_to_feature_vector_index(self, rule_indexes: list):
		'''
		'''
		index = 0

		for i, rule_num in enumerate(rule_indexes[::-1]):
			index += ((self.number_of_rules ** i) * rule_num)

		return index

	def feature_vector_setup(self, benign_directory: str, malicious_directory: str, file_name='rule_order.json'):
		"""Finds the feature dictionaries of each file in the benign and 
		malicious directories with the given file extensions

		Args:
			benign_directory (str): Directory containing the benign files
			malicious_directory (str): Directory containing the malicious files
			file_name (str): Name of to use for feature

		Returns:
			Nothing. Populates class instance variables
				self.feature_set: [[]] List of list of features
				self.labels = [] Labels for each list of features in feature_set
				self.feature_order = [] Order in which features appear in feature_set
		"""
		# Get benign code file paths
		benign_file_paths = []
		for root, dirs, files in os.walk(benign_directory):
			for file in files:
				if file == file_name:
					benign_file_paths.append(os.path.join(root, file))

		# Get malicious code file paths
		malicious_file_paths = []
		for root, dirs, files in os.walk(malicious_directory):
			for file in files:
				if file == file_name:
					malicious_file_paths.append(os.path.join(root, file))

		self.feature_set = np.empty((len(benign_file_paths) + len(malicious_file_paths),
			self.number_of_features), bool)

		with progressbar.ProgressBar(max_value=len(benign_file_paths) + len(malicious_file_paths)) as bar:
			self.count = 0
			if self.show_progress_bar:
				bar.update(0)

			threads = []
			for i in range(self.max_thread_count):
				t = Thread(target=self._feature_set_mapper_queue)
				t.start()
				threads.append(t)

			# For each file
			for row, file_path in enumerate(benign_file_paths + malicious_file_paths):
				# Open file
				with open(file_path, 'r') as file:
					# Read in json data as a list
					data = json.load(file)
					item = {'data': data, 'row': row, 'bar': bar}
					# Add to threading queue
					self.queue.put(item)

			# Wait until all tasks are finished
			self.queue.join()

			# Stop threads
			for i in range(self.max_thread_count):
				self.queue.put(None)
			for t in threads:
				t.join()

		# Add labels (benign = 0, malicious = 1)
		self.labels = np.zeros(len(benign_file_paths) + len(malicious_file_paths))
		for i in range(len(benign_file_paths), self.labels.size):
			self.labels[i] = 1
		###################################

	def _feature_set_mapper_queue(self):
		while True:
			arguments = self.queue.get()
			if arguments is None:
				break
			data = arguments['data']
			row = arguments['row']
			bar = arguments['bar']

			# Clean up data
			data_num = []
			# For each item in data
			for x in data:
				# Convert to lowercase
				x = x.lower()
				# If not an ignored rule
				if x not in self.bad_rules:
					# If rule is being trimmed
					if x in self.trimmed_rules:
						# Changed rule to trimmed rule
						x = self.trimmed_rules[x]
					# Add numerical form of data
					data_num.append(self.rule_mapping[x])

			feature_vector = self._feature_set_mapper(data_num)

			self.mutex.acquire()
			try:
				# Add feature vector to feature matrix
				for col, feature in enumerate(feature_vector):
					self.feature_set[row, col] = feature

				self.count += 1
				if self.show_progress_bar:
					bar.update(self.count)
			finally:
				self.mutex.release()

			self.queue.task_done()

	def _feature_set_mapper(self, data: list):
		'''
		self.n_gram_setup should be called first
		'''
		# Fill feature vector with zeros
		feature_vector = np.zeros(self.number_of_features, dtype=bool)

		# For each n-gram
		for index in range(len(data) - self.n + 1):
			# Get n-gram features
			n_gram = []
			for i in range(self.n):
				n_gram.append(data[index + i])
			# Convert to hash-able type
			n_gram = tuple(n_gram)

			# Find n-gram position in feature vector and set bit
			feature_vector[self._rule_to_feature_vector_index(n_gram)] = True

		return feature_vector


	def pickle_it(self, directory, save_file):
		import pickle
		if not os.path.exists(directory):
			os.makedirs(directory)

		path = directory + '/' + save_file + '__'

		pickle.dump(self.feature_set, open(path + "feature_set.p", "wb"))
		pickle.dump(self.labels, open(path + "feature_labels.p", "wb"))
		print("Pickled Success!")

	def unpickle_it(self, directory):
		import pickle
		self.feature_set = pickle.load(open(directory + "/feature_set.p", "rb"))
		self.labels = pickle.load(open(directory + "/feature_labels.p", "rb"))

	def _cross_val_score_queue(self):
		while True:
			arguments = self.queue.get()
			if arguments is None:
				break
			clf = deepcopy(arguments['clf'])
			train_index = arguments['train_index']
			test_index = arguments['test_index']

			x_train, x_test = self.feature_set[train_index], self.feature_set[test_index]
			y_train, y_test = self.labels[train_index], self.labels[test_index]

			clf.fit(x_train, y_train)
			prediction = clf.predict(x_test)

			tn, fp, fn, tp = confusion_matrix(y_test, prediction).ravel()

			self.mutex.acquire()
			try:
				self.tn_lst.append(tn)
				self.fp_lst.append(fp)
				self.fn_lst.append(fn)
				self.tp_lst.append(tp)
			finally:
				self.mutex.release()

			self.queue.task_done()

	def _cross_val_score(self, clf, algorithm):
		kf = KFold(n_splits=self.splits, shuffle=True)

		self.tn_lst = []
		self.fp_lst = []
		self.fn_lst = []
		self.tp_lst = []

		self.mutex = Lock()
		self.queue = queue.Queue()

		threads = []
		for i in range(self.max_thread_count):
			t = Thread(target=self._cross_val_score_queue)
			t.start()
			threads.append(t)

		for train_index, test_index in kf.split(self.feature_set):
			item = {'clf': clf, 'train_index': train_index, 'test_index': test_index}
			self.queue.put(item)

		# Wait until all tasks are finished
		self.queue.join()

		# Stop threads
		for i in range(self.max_thread_count):
			self.queue.put(None)
		for t in threads:
			t.join()

		tn = mean(self.tn_lst)
		fp = mean(self.fp_lst)
		fn = mean(self.fn_lst)
		tp = mean(self.tp_lst)

		recall = tp / (tp + fn)
		precision =  tp / (tp + fp)
		f1_score = 2 * (precision * recall) / (precision + recall)

		with open(self.save_file, 'a') as f:
			f.write('\n')
			f.write('Algorithm: {}\n'.format(algorithm))
			f.write('{}-gram\n'.format(self.n))
			if self.feature_selection == True:
				f.write('Feature Selection Applied\n')
			f.write('K-Folds: {}\n'.format(self.splits))
			f.write('Total Samples: {}\n'.format(self.labels.size))
			f.write('Total Malicious Samples: {}\n'.format(np.count_nonzero(self.labels == 1)))
			f.write('Total Benign Samples: {}\n'.format(np.count_nonzero(self.labels == 0)))
			f.write('Total Features: {}\n'.format(np.size(self.feature_set, 1)))
			f.write('True Positive: {}\n'.format(tp))
			f.write('False Positive: {}\n'.format(fp))
			f.write('True Negative: {}\n'.format(tn))
			f.write('False Negative: {}\n'.format(fn))
			f.write('Recall: {}\n'.format(recall))
			f.write('Precision: {}\n'.format(precision))
			f.write('F1 Score: {}\n'.format(f1_score))
			f.write('\n')


	def svm(self):
		from sklearn import svm

		clf = svm.SVC()
		self._cross_val_score(clf, 'SVM')

	def naive_bayes(self):
		from sklearn.naive_bayes import GaussianNB

		gnd = GaussianNB()
		self._cross_val_score(gnd, 'Naive Bayes')

	def decision_tree(self):
		from sklearn import tree

		clf = tree.DecisionTreeClassifier()
		self._cross_val_score(clf, 'Decision Tree')

	def random_forest(self):
		from sklearn.ensemble import RandomForestClassifier

		clf = RandomForestClassifier(n_estimators=100)
		self._cross_val_score(clf, 'Random Forest')

	def neural_network(self):
		from sklearn.neural_network import MLPClassifier

		# For small datasets, however, ‘lbfgs’ can converge faster and perform better.
		clf = MLPClassifier(solver='lbfgs', alpha=1e-5, \
							hidden_layer_sizes=(5,2), random_state=1)
		self._cross_val_score(clf, 'Neural Network')

	def stochastic_gradient_descent(self):
		from sklearn.linear_model import SGDClassifier
		clf = SGDClassifier(max_iter=1000, tol=1e-3)
		self._cross_val_score(clf, 'Stochastic Gradient Descent Classifier')

	def knn(self):
		from sklearn.neighbors import KNeighborsClassifier
		clf = KNeighborsClassifier(weights='distance')
		self._cross_val_score(clf, 'K-Nearest Neighbors')


def ml(show_pb=False, pickle_dir='./pickle'):
	save_file = './results/results_full_features.txt'
	if not os.path.exists('./results/'):
		os.makedirs('./results/')
	with open(save_file, 'a') as f:
		f.write('\n-------------------------------------------------------------------------\n')

	for n in range(1, 3):
		# Non-Feature Reduction
		ml = MachineLearning(n_grams=n, show_progress_bar=show_pb, save_file=save_file)
		ml.setup(benign_directory='./benign_samples', malicious_directory='./malicious_samples')
		#ml.setup(benign_directory='/tmp/1', malicious_directory='/tmp/2')


		ml.pickle_it(pickle_dir, 'ml__n_{}__feature_selection_{}'.format(n, False))
	
		ml.decision_tree()
		#'''
		ml.random_forest()
		ml.neural_network()
		ml.naive_bayes()
		ml.stochastic_gradient_descent()
		ml.knn()
		ml.svm()
		#'''


		# Feature Reduction
		ml.feature_selection_do()
		ml.pickle_it(pickle_dir, 'ml__n_{}__feature_selection_{}'.format(n, True))
	
		ml.decision_tree()
		#'''
		ml.random_forest()
		ml.neural_network()
		ml.naive_bayes()
		ml.stochastic_gradient_descent()
		ml.knn()
		ml.svm()
		#'''

def trimmedML(show_pb=False, pickle_dir='./pickle'):
	# Trimmed rules
	trimmed_rules = {}
	
	trimmed_rules['DoStatement'.lower()] = 'do_for_while_statement'
	trimmed_rules['WhileStatement'.lower()] = 'do_for_while_statement'
	trimmed_rules['ForStatement'.lower()] = 'do_for_while_statement'
	trimmed_rules['ForVarStatement'.lower()] = 'do_for_while_statement'
	trimmed_rules['ForInStatement'.lower()] = 'do_for_while_statement'
	trimmed_rules['ForVarInStatement'.lower()] = 'do_for_while_statement'
	trimmed_rules['PropertyExpressionAssignment'.lower()] = 'property_rules'
	trimmed_rules['ComputedPropertyExpressionAssignment'.lower()] = 'property_rules'
	trimmed_rules['PropertyGetter'.lower()] = 'property_rules'
	trimmed_rules['PropertySetter'.lower()] = 'property_rules'
	trimmed_rules['MethodProperty'.lower()] = 'property_rules'
	trimmed_rules['PropertyShorthand'.lower()] = 'property_rules'
	trimmed_rules['FunctionExpression'.lower()] = 'expression_statement'
	trimmed_rules['ClassExpression'.lower()] = 'expression_statement'
	trimmed_rules['MemberIndexExpression'.lower()] = 'expression_statement'
	trimmed_rules['MemberDotExpression'.lower()] = 'expression_statement'
	trimmed_rules['ArgumentsExpression'.lower()] = 'expression_statement'
	trimmed_rules['NewExpression'.lower()] = 'expression_statement'
	trimmed_rules['PostIncrementExpression'.lower()] = 'expression_statement'
	trimmed_rules['PostDecreaseExpression'.lower()] = 'expression_statement'
	trimmed_rules['DeleteExpression'.lower()] = 'expression_statement'
	trimmed_rules['VoidExpression'.lower()] = 'expression_statement'
	trimmed_rules['TypeofExpression'.lower()] = 'expression_statement'
	trimmed_rules['PreIncrementExpression'.lower()] = 'expression_statement'
	trimmed_rules['PreDecreaseExpression'.lower()] = 'expression_statement'
	trimmed_rules['UnaryPlusExpression'.lower()] = 'expression_statement'
	trimmed_rules['UnaryMinusExpression'.lower()] = 'expression_statement'
	trimmed_rules['BitNotExpression'.lower()] = 'expression_statement'
	trimmed_rules['NotExpression'.lower()] = 'expression_statement'
	trimmed_rules['MultiplicativeExpression'.lower()] = 'expression_statement'
	trimmed_rules['AdditiveExpression'.lower()] = 'expression_statement'
	trimmed_rules['BitShiftExpression'.lower()] = 'expression_statement'
	trimmed_rules['RelationalExpression'.lower()] = 'expression_statement'
	trimmed_rules['InstanceofExpression'.lower()] = 'expression_statement'
	trimmed_rules['InExpression'.lower()] = 'expression_statement'
	trimmed_rules['EqualityExpression'.lower()] = 'expression_statement'
	trimmed_rules['BitAndExpression'.lower()] = 'expression_statement'
	trimmed_rules['BitXOrExpression'.lower()] = 'expression_statement'
	trimmed_rules['BitOrExpression'.lower()] = 'expression_statement'
	trimmed_rules['LogicalAndExpression'.lower()] = 'expression_statement'
	trimmed_rules['LogicalOrExpression'.lower()] = 'expression_statement'
	trimmed_rules['TernaryExpression'.lower()] = 'expression_statement'
	trimmed_rules['AssignmentExpression'.lower()] = 'expression_statement'
	trimmed_rules['AssignmentOperatorExpression'.lower()] = 'expression_statement'
	trimmed_rules['TemplateStringExpression'.lower()] = 'expression_statement'
	trimmed_rules['ThisExpression'.lower()] = 'expression_statement'
	trimmed_rules['IdentifierExpression'.lower()] = 'expression_statement'
	trimmed_rules['SuperExpression'.lower()] = 'expression_statement'
	trimmed_rules['LiteralExpression'.lower()] = 'expression_statement'
	trimmed_rules['ArrayLiteralExpression'.lower()] = 'expression_statement'
	trimmed_rules['ObjectLiteralExpression'.lower()] = 'expression_statement'
	trimmed_rules['ParenthesizedExpression'.lower()] = 'expression_statement'
	trimmed_rules['ArrowFunctionExpression'.lower()] = 'expression_statement'

	save_file = './results/results_trimmed_features.txt'

	if not os.path.exists('./results/'):
		os.makedirs('./results/')
	with open(save_file, 'a') as f:
		f.write('\n-------------------------------------------------------------------------\n')

	for n in range(1, 5):
		# Non-Feature Reduction
		ml = MachineLearning(n_grams=n, show_progress_bar=show_pb, save_file=save_file, trimmed_rules=trimmed_rules)
		ml.setup(benign_directory='./benign_samples', malicious_directory='./malicious_samples')
		#ml.setup(benign_directory='/tmp/1', malicious_directory='/tmp/2')

		ml.pickle_it(pickle_dir, 'ml_trimmed__n_{}__feature_selection_{}'.format(n, False))
	
		ml.decision_tree()
		#'''
		ml.random_forest()
		ml.neural_network()
		ml.naive_bayes()
		ml.stochastic_gradient_descent()
		ml.knn()
		ml.svm()
		#'''

		# Feature Reduction
		ml.feature_selection_do()
		ml.pickle_it(pickle_dir, 'ml_trimmed__n_{}__feature_selection_{}'.format(n, True))

	
		ml.decision_tree()
		#'''
		ml.random_forest()
		ml.neural_network()
		ml.naive_bayes()
		ml.stochastic_gradient_descent()
		ml.knn()
		ml.svm()
		#'''


def main():
	show_progress_bar = False
	pickle_dir = './pickle'
	#ml(show_progress_bar)
	trimmedML(show_progress_bar, pickle_dir)

			
	'''
	ml.feature_vector_setup('/home/drc/Desktop/Research/grammar_malware_analysis/antlr/malicious_samples/1',
		'/home/drc/Desktop/Research/grammar_malware_analysis/antlr/malicious_samples/1',
		'rule_order.json')
	'''

	'''
	import argparse

	parser = argparse.ArgumentParser()
	parser.add_argument("parser", help="Path to parser to use", type=str)
	parser.add_argument("benign_directory", help="Directory containing the benign files", type=str)
	parser.add_argument("malicious_directory", help="Directory containing the malicious files", type=str)
	parser.add_argument("file_extensions", help="Comma delimited list of acceptable file extensions", type=str)
	args = parser.parse_args()

	extensions = []
	for ext in args.file_extensions.split(','):
		extensions.append('.{}'.format(ext))
	args.file_extensions = extensions

	ml = machineLearning(args.parser)
	
	ml.feature_vector_setup(args.benign_directory, args.malicious_directory, args.file_extensions);
	#print(ml.feature_set)
	ml.pickle_it()
	#ml.unpickle_it()
	'''

	'''
	ml.svm()
	ml.naive_bayes()
	ml.decision_tree()
	ml.random_forest()
	ml.neural_network()
	'''


if __name__ == '__main__':
    main()