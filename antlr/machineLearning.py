import argparse
import os
import io
import sys
import progressbar
from threading import Thread, Lock
import pandas
import subprocess
import json
import queue
from copy import deepcopy
import numpy as np

"""
# NOTE ************************************************
# If number of times each grammar rule isn't useful, try percentage
# of how often the rule is used in the file (ie, normalize the data)

* Use tokens as a rule set as well
########################################################
"""

class machineLearning:
	def __init__(self):
		self.feature_order = []
		self.feature_set = np.array(0)
		self.labels = np.array(0)

		self.benign_count = 0
		self.malicious_count = 0

	def setup(self, benign_directory, malicious_directory, file_name='rule_order.json', 
		n=4, rule_file='./JavaScript/js_possible_rules.txt'):
		'''
		'''
		self.n_gram_setup(n, rule_file)
		self.feature_vector_setup(benign_directory, malicious_directory, file_name)

	def n_gram_setup(self, n=4, rule_file='./JavaScript/js_possible_rules.txt'):
		self.n = n
		# List of rules
		rules = []
		self.bad_rules = set()
		# Open rules file
		with open(rule_file, 'r') as f:
			for line in f:
				# Remove rules commented out with '#'
				if not line.startswith('#'):
					rules.append(line.strip().lower())
				else:
					self.bad_rules.add((line[1:]).strip().lower())

		self.n_gram = set()
		self._recursive_n_gram(self.n, rules, [])
		'''
		for rule1 in rules:
			for rule2 in rules:
				for rule3 in rules:
					for rule4 in rules:
						self.n_gram.add((rule1, rule2, rule3, rule4))
		'''
		# Mapping from n-gram tuple to index in feature set array
		self.n_gram_dict = {}
		for index, gram in enumerate(self.n_gram):
			self.n_gram_dict[gram] = index

	def _recursive_n_gram(self, n: int, rules: list, rule_to_add_to: list):
		# For each rule
		for rule in rules:
			# Make a copy of the rule and add element
			rule_with_addition = deepcopy(rule_to_add_to)
			rule_with_addition.append(rule)
			# If this is the last rule to add, add tuple to n-gram set
			if n == 1:
				self.n_gram.add(tuple(rule_with_addition))
			else:
				# Recursively add more elements to the tuple
				self._recursive_n_gram(n-1, rules, rule_with_addition)


	def feature_vector_setup(self, benign_directory: str, malicious_directory: str, file_name='rule_order.json'):
		"""Finds the feature dictionaries of each file in the benign and 
		malicious directories with the given file extensions

		Args:
			benign_directory (str): Directory containing the benign files
			malicious_directory (str): Directory containing the malicious files
			file_name (str): Name of to use for feature

		Returns:
			Nothing. Populates class instance variables
				self.feature_set: [[]] List of list of features
				self.labels = [] Labels for each list of features in feature_set
				self.feature_order = [] Order in which features appear in feature_set
		"""
		# Get benign code file paths
		benign_file_paths = []
		for root, dirs, files in os.walk(benign_directory):
			for file in files:
				if file == file_name:
					benign_file_paths.append(os.path.join(root, file))

		# Get malicious code file paths
		malicious_file_paths = []
		for root, dirs, files in os.walk(malicious_directory):
			for file in files:
				if file == file_name:
					malicious_file_paths.append(os.path.join(root, file))

		self.feature_set = np.empty((len(benign_file_paths) + len(malicious_file_paths),
			len(self.n_gram_dict)), bool)

		with progressbar.ProgressBar(max_value=len(benign_file_paths) + len(malicious_file_paths)) as bar:
			count = 0
			bar.update(0)
			# For each file
			for row, file_path in enumerate(benign_file_paths + malicious_file_paths):
				# Open file
				with open(file_path, 'r') as file:
					# Read in json data as a list
					data = json.load(file)
					# Take the lowercase of all the rules
					lower_case_data = [x.lower() for x in data if x.lower() not in self.bad_rules]
					# Get feature vector
					feature_vector = self._feature_set_mapper(lower_case_data)
					# Add feature vector to feature matrix
					for col, feature in enumerate(feature_vector):
						self.feature_set[row, col] = feature

					count += 1
					bar.update(count)

		# Add labels
		self.labels = np.zeros(len(benign_file_paths) + len(malicious_file_paths))
		for i in range(len(benign_file_paths), self.labels.size):
			self.labels[i] = 1
		###################################

	def _feature_set_mapper(self, data: list):
		'''
		self.n_gram_setup should be called first
		'''
		# Fill feature vector with zeros
		feature_vector = np.zeros(len(self.n_gram_dict), dtype=bool)

		# For each n-gram
		for index in range(len(data) - self.n + 1):
			# Get n-gram features
			n_gram = []
			for i in range(self.n):
				n_gram.append(data[index + i])
			# Convert to hash-able type
			n_gram = tuple(n_gram)
			# Find n-gram position in feature vector and set bit
			feature_vector[self.n_gram_dict[n_gram]] = True

		return feature_vector


	def pickle_it(self):
		import pickle
		if not os.path.exists("./pickle/"):
			os.makedirs("./pickle/")
		pickle.dump(self.feature_set, open("./pickle/feature_set.p", "wb"))
		pickle.dump(self.labels, open("./pickle/feature_labels.p", "wb"))
		pickle.dump(self.n_gram_dict, open("./pickle/n_grams.p", "wb"))
		print("Pickled Success!")

	def unpickle_it(self):
		import pickle
		self.feature_set = pickle.load(open("./pickle/feature_set.p", "rb"))
		self.labels = pickle.load(open("./pickle/feature_labels.p", "rb"))
		self.feature_order = pickle.load(open("./pickle/feature_order.p", "rb"))

	def _cross_val_score(self, clf):
		from sklearn.model_selection import cross_val_score

		scores = cross_val_score(clf, self.feature_set, self.labels.ravel(), cv=2)
		print("Accuracy: %0.2f (+/- %0.2f)" % (scores.mean(), scores.std() * 2))


	def svm(self):
		from sklearn import svm

		clf = svm.SVC()
		self._cross_val_score(clf)

	def naive_bayes(self):
		from sklearn.naive_bayes import GaussianNB

		gnd = GaussianNB()
		self._cross_val_score(gnd)

	def decision_tree(self):
		from sklearn import tree

		clf = tree.DecisionTreeClassifier()
		self._cross_val_score(clf)

	def random_forest(self):
		from sklearn.ensemble import RandomForestClassifier

		clf = RandomForestClassifier()
		self._cross_val_score(clf)

	def neural_network(self):
		from sklearn.neural_network import MLPClassifier

		# For small datasets, however, ‘lbfgs’ can converge faster and perform better.
		clf = MLPClassifier(solver='lbfgs', alpha=1e-5, \
							hidden_layer_sizes=(5,2), random_state=1)
		self._cross_val_score(clf)


def main():
	ml = machineLearning()
	ml.setup(benign_directory='/home/drc/Desktop/Research/grammar_malware_analysis/antlr/benign_samples',
		malicious_directory='/home/drc/Desktop/Research/grammar_malware_analysis/antlr/malicious_samples',
		n=2)
	ml.pickle_it()
	'''
	ml.feature_vector_setup('/home/drc/Desktop/Research/grammar_malware_analysis/antlr/malicious_samples/1',
		'/home/drc/Desktop/Research/grammar_malware_analysis/antlr/malicious_samples/1',
		'rule_order.json')
	'''

	'''
	import argparse

	parser = argparse.ArgumentParser()
	parser.add_argument("parser", help="Path to parser to use", type=str)
	parser.add_argument("benign_directory", help="Directory containing the benign files", type=str)
	parser.add_argument("malicious_directory", help="Directory containing the malicious files", type=str)
	parser.add_argument("file_extensions", help="Comma delimited list of acceptable file extensions", type=str)
	args = parser.parse_args()

	extensions = []
	for ext in args.file_extensions.split(','):
		extensions.append('.{}'.format(ext))
	args.file_extensions = extensions

	ml = machineLearning(args.parser)
	
	ml.feature_vector_setup(args.benign_directory, args.malicious_directory, args.file_extensions);
	#print(ml.feature_set)
	ml.pickle_it()
	#ml.unpickle_it()
	'''

	'''
	ml.svm()
	ml.naive_bayes()
	ml.decision_tree()
	ml.random_forest()
	ml.neural_network()
	'''


if __name__ == '__main__':
    main()