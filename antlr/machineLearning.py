import argparse
import os
import io
import sys
import progressbar
from threading import Thread, Lock
import pandas
import subprocess
import json

"""
# NOTE ************************************************
# If number of times each grammar rule isn't useful, try percentage
# of how often the rule is used in the file (ie, normalize the data)

* Use tokens as a rule set as well
########################################################
"""

class machineLearning:
	def __init__(self, parser_type):
		self.parser = parser_type
		self.mutex = Lock()

		self.feature_order = []
		self.feature_set = pandas.DataFrame()
		self.labels = pandas.Series()

		self.benign_count = 0
		self.malicious_count = 0

		# Clear failed_files list
		open('./.failed_files.txt', 'w').close()

		# Generate temporary directory
		self.tmp_dir = './.tmp/'
		if not os.path.exists(self.tmp_dir):
			os.makedirs(self.tmp_dir)
		self.tmp_dir = os.path.abspath(self.tmp_dir) + '/'

	def feature_vector_setup(self, benign_directory: str, malicious_directory: str, file_extensions: list):
		"""Finds the feature dictionaries of each file in the benign and 
		malicious directories with the given file extensions

		Args:
			benign_directory (str): Directory containing the benign files
			malicious_directory (str): Directory containing the malicious files
			file_extensions ([str]): Acceptable file extensions

		Returns:
			Nothing. Sets two class instance variables
				self.feature_set: [[]] List of list of features
				self.labels = [] Labels for each list of features in feature_set
				self.feature_order = [] Order in which features appear in feature_set
		"""
		# Get benign code file paths
		benign_file_paths = []
		for root, dirs, files in os.walk(benign_directory):
			for file in files:
				for file_extension in file_extensions:
					if file.endswith(file_extension):
						benign_file_paths.append(os.path.join(root, file))

		# Get malicious code file paths
		malicious_file_paths = []
		for root, dirs, files in os.walk(malicious_directory):
			for file in files:
				for file_extension in file_extensions:
					if file.endswith(file_extension):
						malicious_file_paths.append(os.path.join(root, file))

		##### Parsing Benign Files #####
		# Holds parsing threads
		threads = []
		# Progress bar
		count = -1
		bar = self._progress_bar(len(benign_file_paths), "Benign File Parsing")
		# For each file
		for index, file in enumerate(benign_file_paths):
			# Multi-thread parsing
			threads.append(Thread(target = self._find_feature_dictionary, \
				args = (file, "Benign", str(index))))
			# Start thread
			threads[-1].start()

		# Join threads, update progress bar
		for thread in threads:
			thread.join()
			count += 1
			bar.update(count + 1)			
		bar.finish()
		################################

		##### Parsing Malicious Files #####
		# Reset parsing threads
		threads = []
		# Progress bar
		count = -1
		bar = self._progress_bar(len(benign_file_paths), "Malicious File Parsing")
		# For each file
		for index, file in enumerate(benign_file_paths):
			# Multi-thread parsing
			threads.append(Thread(target = self._find_feature_dictionary, \
				args = (file, "Malicious", str(index))))
			# Start thread
			threads[-1].start()

		# Join threads, update progress bar
		for thread in threads:
			thread.join()
			count += 1
			bar.update(count + 1)			
		bar.finish()

		sys.stderr = sys.stdout

		# Add labels
		self.labels = pandas.Series(data=((["Benign"] * self.benign_count) + (["Malicious"] * self.malicious_count)))
		###################################

	def _progress_bar(self, size: int, name: str):
		"""Returns a progress bar
		Args:
			size (int): Max size of progress bar (i.e. 20/20)
			name (str): What the progress bar is for
		"""
		print("Starting {}".format(name))
		bar = progressbar.ProgressBar(maxval = size, fd=sys.stdout, \
			widgets=[progressbar.Bar('=', '[', ']'), ' ', progressbar.Percentage()])
		bar.start()

		return bar

	def _find_feature_dictionary(self, file: str, label: str, id: int, tokens_or_rules='both'):
		"""Find the feature dictionary of the specified file

		Supports multi-threading. Locks when adding features_dict to
		feature_dictionaries list.

		Uses self.mutex to lock resource.

		Args:
			file (str): File to parse and find the feature dictionary for
			label (str): Label for the current file being parsed
			id (str): Unique id to save a temporary json file to
			tokens_or_rules (str): Use tokens or rules in feature set
				* 'both': Both
				* 'tokens': Just tokens
				* 'rules': Just rules

		Returns:
			Nothing. Appends features to feature_dictionaries list.
		"""
		# Run parser
		token_file_name = self.tmp_dir + 'token_{}.txt'.format(id)
		rule_file_name = self.tmp_dir + 'rule_{}.txt'.format(id)
		subprocess.run(['node', self.parser, file, 
			token_file_name, 
			rule_file_name])

		token_dict = {}
		rule_dict = {}
		# If file was parsed and json files were saved, read in json files
		if os.path.exists(token_file_name):
			with open(token_file_name) as json_file:  
				token_dict = json.load(json_file)
			with open(rule_file_name) as json_file:  
				rule_dict = json.load(json_file)
			# Remove tmp files
			subprocess.run(['rm', token_file_name, rule_file_name])
		
		# Feature set
		features = None
		if tokens_or_rules == 'tokens':
			features = token_dict
		elif tokens_or_rules == 'rules':
			features = rule_dict
		else:
			features = {**token_dict, ** rule_dict}

		# Lock mutex
		self.mutex.acquire()
		try:
			# If file was parsed
			if len(features) != 0:
				# If no elements in feature_set, get order of fields
				if len(self.feature_order) == 0:
					for key in features:
						self.feature_order.append(key)
					self.feature_set = pandas.DataFrame(columns=self.feature_order)
				# Append rules to data-frame
				self.feature_set = self.feature_set.append(features, ignore_index=True)
				# Keep track of benign and malicious count (for labels)
				if label.lower() == "benign":
					self.benign_count += 1
				else:
					self.malicious_count += 1
			else:
				with open("./.failed_files.txt", "a") as f:
					f.write("{}\n".format(file))
		except Exception as e: 
			print(e)
			#print("Append rule usage to feature_dictionaries FAILED")
		finally:
			self.mutex.release()


	def pickle_it(self):
		import pickle
		if not os.path.exists("./pickle/"):
			os.makedirs("./pickle/")
		pickle.dump(self.feature_set, open("./pickle/feature_set.p", "wb"))
		pickle.dump(self.labels, open("./pickle/feature_labels.p", "wb"))
		pickle.dump(self.feature_order, open("./pickle/feature_order.p", "wb"))
		print("Pickled Success!")

	def unpickle_it(self):
		import pickle
		self.feature_set = pickle.load(open("./pickle/feature_set.p", "rb"))
		self.labels = pickle.load(open("./pickle/feature_labels.p", "rb"))
		self.feature_order = pickle.load(open("./pickle/feature_order.p", "rb"))

	def _cross_val_score(self, clf):
		from sklearn.model_selection import cross_val_score

		scores = cross_val_score(clf, self.feature_set, self.labels.ravel(), cv=2)
		print("Accuracy: %0.2f (+/- %0.2f)" % (scores.mean(), scores.std() * 2))


	def svm(self):
		from sklearn import svm

		clf = svm.SVC()
		self._cross_val_score(clf)

	def naive_bayes(self):
		from sklearn.naive_bayes import GaussianNB

		gnd = GaussianNB()
		self._cross_val_score(gnd)

	def decision_tree(self):
		from sklearn import tree

		clf = tree.DecisionTreeClassifier()
		self._cross_val_score(clf)

	def random_forest(self):
		from sklearn.ensemble import RandomForestClassifier

		clf = RandomForestClassifier()
		self._cross_val_score(clf)

	def neural_network(self):
		from sklearn.neural_network import MLPClassifier

		# For small datasets, however, ‘lbfgs’ can converge faster and perform better.
		clf = MLPClassifier(solver='lbfgs', alpha=1e-5, \
							hidden_layer_sizes=(5,2), random_state=1)
		self._cross_val_score(clf)


def main():
	import argparse

	parser = argparse.ArgumentParser()
	parser.add_argument("parser", help="Path to parser to use", type=str)
	parser.add_argument("benign_directory", help="Directory containing the benign files", type=str)
	parser.add_argument("malicious_directory", help="Directory containing the malicious files", type=str)
	parser.add_argument("file_extensions", help="Comma delimited list of acceptable file extensions", type=str)
	args = parser.parse_args()

	extensions = []
	for ext in args.file_extensions.split(','):
		extensions.append('.{}'.format(ext))
	args.file_extensions = extensions

	ml = machineLearning(args.parser)
	
	ml.feature_vector_setup(args.benign_directory, args.malicious_directory, args.file_extensions);
	print(ml.feature_set)
	'''
	ml.pickle_it()
	#ml.unpickle_it()

	ml.svm()
	ml.naive_bayes()
	ml.decision_tree()
	ml.random_forest()
	ml.neural_network()
	'''


if __name__ == '__main__':
    main()