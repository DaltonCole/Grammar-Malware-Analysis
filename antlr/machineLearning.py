import argparse
import os
import io
import sys
import progressbar
from threading import Thread, Lock
import pandas

"""
# NOTE ************************************************
# If number of times each grammar rule isn't useful, try percentage
# of how often the rule is used in the file (ie, normalize the data)

* Use tokens as a rule set as well
########################################################
"""

class machineLearning:
	def __init__(self, parser_type):
		self.parser = parser_type()
		self.mutex = Lock()

		self.feature_order = []
		self.feature_set = pandas.DataFrame()
		self.labels = pandas.Series()

		self.benign_count = 0
		self.malicious_count = 0

		# Clear failed_files list
		open('./.failed_files.txt', 'w').close()

	def feature_vector_setup(self, benign_directory: str, malicious_directory: str, file_extensions: list):
		"""Finds the feature dictionaries of each file in the benign and 
		malicious directories with the given file extensions

		Args:
			benign_directory (str): Directory containing the benign files
			malicious_directory (str): Directory containing the malicious files
			file_extensions ([str]): Acceptable file extensions

		Returns:
			Nothing. Sets two class instance variables
				self.feature_set: [[]] List of list of features
				self.labels = [] Labels for each list of features in feature_set
				self.feature_order = [] Order in which features appear in feature_set
		"""
		# Get benign code file paths
		benign_file_paths = []
		for root, dirs, files in os.walk(benign_directory):
			for file in files:
				for file_extension in file_extensions:
					if file.endswith(file_extension) and len(benign_file_paths) < 1:
						benign_file_paths.append(os.path.join(root, file))

		# Get malicious code file paths
		malicious_file_paths = []
		for root, dirs, files in os.walk(malicious_directory):
			for file in files:
				for file_extension in file_extensions:
					if file.endswith(file_extension) and len(malicious_file_paths) < 1:
						malicious_file_paths.append(os.path.join(root, file))

		##### Parsing Benign Files #####
		# Holds parsing threads
		threads = []
		# Progress bar
		count = -1
		bar = self._progress_bar(len(benign_file_paths), "Benign File Parsing")
		# For each file
		for file in benign_file_paths:
			# Multi-thread parsing
			threads.append(Thread(target = self._find_feature_dictionary, \
				args = (file, "Benign")))
			# Start thread
			threads[-1].start()

		# Join threads, update progress bar
		for thread in threads:
			thread.join()
			count += 1
			bar.update(count + 1)			
		bar.finish()
		################################

		##### Parsing Malicious Files #####
		# Reset parsing threads
		threads = []
		# Progress bar
		count = -1
		bar = self._progress_bar(len(benign_file_paths), "Malicious File Parsing")
		# For each file
		for file in benign_file_paths:
			# Multi-thread parsing
			threads.append(Thread(target = self._find_feature_dictionary, \
				args = (file, "Malicious")))
			# Start thread
			threads[-1].start()

		# Join threads, update progress bar
		for thread in threads:
			thread.join()
			count += 1
			bar.update(count + 1)			
		bar.finish()

		sys.stderr = sys.stdout

		# Add labels
		self.labels = pandas.Series(data=((["Benign"] * self.benign_count) + (["Malicious"] * self.malicious_count)))
		###################################

	def _progress_bar(self, size: int, name: str):
		"""Returns a progress bar
		Args:
			size (int): Max size of progress bar (i.e. 20/20)
			name (str): What the progress bar is for
		"""
		print("Starting {}".format(name))
		bar = progressbar.ProgressBar(maxval = size, fd=sys.stdout, \
			widgets=[progressbar.Bar('=', '[', ']'), ' ', progressbar.Percentage()])
		bar.start()

		return bar

	def _find_feature_dictionary(self, file: str, label: str):
		"""Find the feature dictionary of the specified file

		Supports multi-threading. Locks when adding features_dict to
		feature_dictionaries list.

		Uses self.mutex to lock resource.

		Args:
			file (str): File to parse and find the feature dictionary for
			label ([{}]): Label for the current file being parsed

		Returns:
			Nothing. Appends features to feature_dictionaries list.
		"""
		# Get rules and rule usage
		rule_usage = self.parser.run_sample(file)

		self.mutex.acquire()
		try:
			if rule_usage != None:
				# If no elements in feature_set, get order of fields
				if len(self.feature_order) == 0:
					for key in rule_usage:
						self.feature_order.append(key)
					self.feature_set = pandas.DataFrame(columns=self.feature_order)
				# Append rules to data-frame
				self.feature_set = self.feature_set.append(rule_usage, ignore_index=True)
				# Keep track of benign and malicious count (for labels)
				if label.lower() == "benign":
					self.benign_count += 1
				else:
					self.malicious_count += 1
			else:
				with open("./.failed_files.txt", "a") as f:
					f.write("Failed file: {}".format(file))
		except Exception as e: 
			print(e)
			#print("Append rule usage to feature_dictionaries FAILED")
		finally:
			self.mutex.release()

	def pickle_it(self):
		import pickle
		if not os.path.exists("./pickle/"):
			os.makedirs("./pickle/")
		pickle.dump(self.feature_set, open("./pickle/feature_set.p", "wb"))
		pickle.dump(self.labels, open("./pickle/feature_labels.p", "wb"))
		pickle.dump(self.feature_order, open("./pickle/feature_order.p", "wb"))
		print("Pickled Success!")

	def unpickle_it(self):
		import pickle
		self.feature_set = pickle.load(open("./pickle/feature_set.p", "rb"))
		self.labels = pickle.load(open("./pickle/feature_labels.p", "rb"))
		self.feature_order = pickle.load(open("./pickle/feature_order.p", "rb"))

	def _cross_val_score(self, clf):
		from sklearn.model_selection import cross_val_score

		scores = cross_val_score(clf, self.feature_set, self.labels.ravel(), cv=2)
		print("Accuracy: %0.2f (+/- %0.2f)" % (scores.mean(), scores.std() * 2))


	def svm(self):
		from sklearn import svm

		clf = svm.SVC()
		self._cross_val_score(clf)

	def naive_bayes(self):
		from sklearn.naive_bayes import GaussianNB

		gnd = GaussianNB()
		self._cross_val_score(gnd)

	def decision_tree(self):
		from sklearn import tree

		clf = tree.DecisionTreeClassifier()
		self._cross_val_score(clf)

	def random_forest(self):
		from sklearn.ensemble import RandomForestClassifier

		clf = RandomForestClassifier()
		self._cross_val_score(clf)

	def neural_network(self):
		from sklearn.neural_network import MLPClassifier

		# For small datasets, however, ‘lbfgs’ can converge faster and perform better.
		clf = MLPClassifier(solver='lbfgs', alpha=1e-5, \
							hidden_layer_sizes=(5,2), random_state=1)
		self._cross_val_score(clf)


def main():
	#from Python3.antlr import Python3ParserHandler
	from C.antlr import CParserHandler

	#ml = ml = machineLearning(Python3ParserHandler)
	ml = machineLearning(CParserHandler)
	#ml.feature_vector_setup(".bad/", "//bad/", [".py"])
	#ml.feature_vector_setup( "./samples/python_samples/small_samples/", "./samples/python_samples/small_samples/",[".py"])
	#ml.feature_vector_setup( "./samples/python_samples/Python-Programs/", "./samples/python_samples/python-scripts/",[".py"])
	ml.feature_vector_setup("./samples/c_samples/benign/", "./samples/cpp_samples/malicious/", [".c", ".h"])
	ml.pickle_it()
	#ml.unpickle_it()

	ml.svm()
	ml.naive_bayes()
	ml.decision_tree()
	ml.random_forest()
	ml.neural_network()


if __name__ == '__main__':
    main()