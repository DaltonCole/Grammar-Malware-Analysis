import argparse
import os
import io
import sys
from threading import Thread, Lock
import json
import queue
from copy import deepcopy
import numpy as np
from sklearn.model_selection import KFold
from sklearn.metrics import confusion_matrix
from statistics import mean
import progressbar
from math import factorial

"""
# NOTE ************************************************
# If number of times each grammar rule isn't useful, try percentage
# of how often the rule is used in the file (ie, normalize the data)

* Use tokens as a rule set as well
########################################################
"""

class machineLearning:
	def __init__(self):
		self.feature_order = []
		self.feature_set = np.array(0)
		self.labels = np.array(0)

		self.benign_count = 0
		self.malicious_count = 0

		self.splits = 10 # k-split cross validation

		self.mutex = Lock()
		self.max_thread_count = 32
		self.queue = queue.Queue()

	def setup(self, benign_directory, malicious_directory, file_name='rule_order.json', 
		n_grams=4, rule_file='./JavaScript/js_possible_rules.txt', show_progress_bar=True,
		feature_selection=False):
		'''
		'''
		self.show_progress_bar = show_progress_bar
		self.feature_selection = feature_selection

		self.n_gram_setup(n_grams, rule_file)
		self.feature_vector_setup(benign_directory, malicious_directory, file_name)

	def n_gram_setup(self, n=4, rule_file='./JavaScript/js_possible_rules.txt'):
		self.n = n

		# List of rules
		rules = []
		self.bad_rules = set()
		# Open rules file
		with open(rule_file, 'r') as f:
			for line in f:
				# Remove rules commented out with '#'
				if not line.startswith('#'):
					rules.append(line.strip().lower())
				else:
					self.bad_rules.add((line[1:]).strip().lower())

		self.rule_mapping = {}
		for index, rule in enumerate(rules):
			self.rule_mapping[rule] = index

		self.number_of_features = len(rules) ** self.n
		self.number_of_rules = len(rules)

	def _rule_to_feature_vector_index(self, rule_indexes: list):
		'''
		'''
		index = 0

		for i, rule_num in enumerate(rule_indexes[::-1]):
			index += ((self.number_of_rules ** i) * rule_num)

		return index

	def feature_vector_setup(self, benign_directory: str, malicious_directory: str, file_name='rule_order.json'):
		"""Finds the feature dictionaries of each file in the benign and 
		malicious directories with the given file extensions

		Args:
			benign_directory (str): Directory containing the benign files
			malicious_directory (str): Directory containing the malicious files
			file_name (str): Name of to use for feature

		Returns:
			Nothing. Populates class instance variables
				self.feature_set: [[]] List of list of features
				self.labels = [] Labels for each list of features in feature_set
				self.feature_order = [] Order in which features appear in feature_set
		"""
		# Get benign code file paths
		benign_file_paths = []
		for root, dirs, files in os.walk(benign_directory):
			for file in files:
				if file == file_name:
					benign_file_paths.append(os.path.join(root, file))

		# Get malicious code file paths
		malicious_file_paths = []
		for root, dirs, files in os.walk(malicious_directory):
			for file in files:
				if file == file_name:
					malicious_file_paths.append(os.path.join(root, file))

		self.feature_set = np.empty((len(benign_file_paths) + len(malicious_file_paths),
			self.number_of_features), bool)

		with progressbar.ProgressBar(max_value=len(benign_file_paths) + len(malicious_file_paths)) as bar:
			self.count = 0
			if self.show_progress_bar:
				bar.update(0)

			threads = []
			for i in range(self.max_thread_count):
				t = Thread(target=self._feature_set_mapper_queue)
				t.start()
				threads.append(t)

			# For each file
			for row, file_path in enumerate(benign_file_paths + malicious_file_paths):
				# Open file
				with open(file_path, 'r') as file:
					# Read in json data as a list
					data = json.load(file)
					item = {'data': data, 'row': row, 'bar': bar}
					# Add to threading queue
					self.queue.put(item)

			# Wait until all tasks are finished
			self.queue.join()

			# Stop threads
			for i in range(self.max_thread_count):
				self.queue.put(None)
			for t in threads:
				t.join()

		# Perform feature selection if set
		if self.feature_selection:
			from sklearn.feature_selection import VarianceThreshold

			sel = VarianceThreshold(threshold=(.8 * (1 - .8)))
			self.feature_set = sel.fit_transform(self.feature_set)

		# Add labels (benign = 0, malicious = 1)
		self.labels = np.zeros(len(benign_file_paths) + len(malicious_file_paths))
		for i in range(len(benign_file_paths), self.labels.size):
			self.labels[i] = 1
		###################################

	def _feature_set_mapper_queue(self):
		while True:
			arguments = self.queue.get()
			if arguments is None:
				break
			data = arguments['data']
			row = arguments['row']
			bar = arguments['bar']

			# Take lowercase and remove bad rules
			# Convert string data to int representation
			data = [self.rule_mapping[x.lower()] for x in data if x.lower() not in self.bad_rules]

			feature_vector = self._feature_set_mapper(data)

			self.mutex.acquire()
			try:
				# Add feature vector to feature matrix
				for col, feature in enumerate(feature_vector):
					self.feature_set[row, col] = feature

				self.count += 1
				if self.show_progress_bar:
					bar.update(self.count)
			finally:
				self.mutex.release()

			self.queue.task_done()

	def _feature_set_mapper(self, data: list):
		'''
		self.n_gram_setup should be called first
		'''
		# Fill feature vector with zeros
		feature_vector = np.zeros(self.number_of_features, dtype=bool)

		# For each n-gram
		for index in range(len(data) - self.n + 1):
			# Get n-gram features
			n_gram = []
			for i in range(self.n):
				n_gram.append(data[index + i])
			# Convert to hash-able type
			n_gram = tuple(n_gram)

			# Find n-gram position in feature vector and set bit
			feature_vector[self._rule_to_feature_vector_index(n_gram)] = True

		return feature_vector


	def pickle_it(self, directory):
		import pickle
		if not os.path.exists(directory):
			os.makedirs(directory)
		pickle.dump(self.feature_set, open(directory + "/feature_set.p", "wb"))
		pickle.dump(self.labels, open(directory + "/feature_labels.p", "wb"))
		print("Pickled Success!")

	def unpickle_it(self, directory):
		import pickle
		self.feature_set = pickle.load(open(directory + "/feature_set.p", "rb"))
		self.labels = pickle.load(open(directory + "/feature_labels.p", "rb"))

	def _cross_val_score_queue(self):
		while True:
			arguments = self.queue.get()
			if arguments is None:
				break
			clf = deepcopy(arguments['clf'])
			train_index = arguments['train_index']
			test_index = arguments['test_index']

			x_train, x_test = self.feature_set[train_index], self.feature_set[test_index]
			y_train, y_test = self.labels[train_index], self.labels[test_index]

			clf.fit(x_train, y_train)
			prediction = clf.predict(x_test)

			tn, fp, fn, tp = confusion_matrix(y_test, prediction).ravel()

			self.mutex.acquire()
			try:
				self.tn_lst.append(tn)
				self.fp_lst.append(fp)
				self.fn_lst.append(fn)
				self.tp_lst.append(tp)
			finally:
				self.mutex.release()

			self.queue.task_done()

	def _cross_val_score(self, clf, algorithm):
		kf = KFold(n_splits=self.splits, shuffle=True)

		self.tn_lst = []
		self.fp_lst = []
		self.fn_lst = []
		self.tp_lst = []

		self.mutex = Lock()
		self.queue = queue.Queue()

		threads = []
		for i in range(self.max_thread_count):
			t = Thread(target=self._cross_val_score_queue)
			t.start()
			threads.append(t)

		for train_index, test_index in kf.split(self.feature_set):
			item = {'clf': clf, 'train_index': train_index, 'test_index': test_index}
			self.queue.put(item)

		# Wait until all tasks are finished
		self.queue.join()

		# Stop threads
		for i in range(self.max_thread_count):
			self.queue.put(None)
		for t in threads:
			t.join()

		tn = mean(self.tn_lst)
		fp = mean(self.fp_lst)
		fn = mean(self.fn_lst)
		tp = mean(self.tp_lst)

		recall = tp / (tp + fn)
		precision =  tp / (tp + fp)
		f1_score = 2 * (precision * recall) / (precision + recall)

		print()
		print('Algorithm: {}'.format(algorithm))
		if self.feature_selection == True:
			print('Feature Selection Applied')
		print('K-Folds: {}'.format(self.splits))
		print('Total Samples: '.format(np.size(self.feature_set, 0)))
		print('Total Malicious Samples: {}'.format(np.count_nonzero(self.labels == 1)))
		print('Total Benign Samples: {}'.format(np.count_nonzero(self.labels == 0)))
		print('Total Features: {}'.format(np.size(self.feature_set, 1)))
		print('True Positive: {}'.format(tp))
		print('False Positive: {}'.format(fp))
		print('True Negative: {}'.format(tn))
		print('False Negative: {}'.format(fn))
		print('Recall: {}'.format(recall))
		print('Precision: {}'.format(precision))
		print('F1 Score: {}'.format(f1_score))
		print()

		with open('./results.txt', 'a') as f:
			f.write('\n')
			f.write('Algorithm: {}'.format(algorithm))
			if self.feature_selection == True:
				f.write('Feature Selection Applied')
			f.write('K-Folds: {}'.format(self.splits))
			f.write('Total Samples: '.format(np.size(self.feature_set, 0)))
			f.write('Total Malicious Samples: {}'.format(np.count_nonzero(self.labels == 1)))
			f.write('Total Benign Samples: {}'.format(np.count_nonzero(self.labels == 0)))
			f.write('Total Features: {}'.format(np.size(self.feature_set, 1)))
			f.write('True Positive: {}'.format(tp))
			f.write('False Positive: {}'.format(fp))
			f.write('True Negative: {}'.format(tn))
			f.write('False Negative: {}'.format(fn))
			f.write('Recall: {}'.format(recall))
			f.write('Precision: {}'.format(precision))
			f.write('F1 Score: {}'.format(f1_score))
			f.write('\n')


	def svm(self):
		from sklearn import svm

		clf = svm.SVC()
		self._cross_val_score(clf, 'SVM')

	def naive_bayes(self):
		from sklearn.naive_bayes import GaussianNB

		gnd = GaussianNB()
		self._cross_val_score(gnd, 'Naive Bayes')

	def decision_tree(self):
		from sklearn import tree

		clf = tree.DecisionTreeClassifier()
		self._cross_val_score(clf, 'Decision Tree')

	def random_forest(self):
		from sklearn.ensemble import RandomForestClassifier

		clf = RandomForestClassifier(n_estimators=100)
		self._cross_val_score(clf, 'Random Forest')

	def neural_network(self):
		from sklearn.neural_network import MLPClassifier

		# For small datasets, however, ‘lbfgs’ can converge faster and perform better.
		clf = MLPClassifier(solver='lbfgs', alpha=1e-5, \
							hidden_layer_sizes=(5,2), random_state=1)
		self._cross_val_score(clf, 'Neural Network')

	def stochastic_gradient_descent(self):
		from sklearn.linear_model import SGDClassifier
		clf = SGDClassifier(max_iter=1000, tol=1e-3)
		self._cross_val_score(clf, 'Stochastic Gradient Descent Classifier')

	def knn(self):
		from sklearn.neighbors import KNeighborsClassifier
		clf = KNeighborsClassifier(weights='distance')
		self._cross_val_score(clf, 'K-Nearest Neighbors')


def main():
	#pickle_string = './pickle_2_gram'

	'''
	ml.setup(benign_directory='/tmp/samples',
		malicious_directory='/tmp/samples2',
		n_grams=2)
	#ml.pickle_it('./pickle_2_gram')
	#quit()
	'''
	'''
	ml.setup(benign_directory='/home/drc/Desktop/Research/grammar_malware_analysis/antlr/benign_samples',
		malicious_directory='/home/drc/Desktop/Research/grammar_malware_analysis/antlr/malicious_samples',
		n_grams=1)
	#ml.pickle_it(pickle_string)
	'''
	#ml.unpickle_it(pickle_string)
	

	#ml.unpickle_it(pickle_string)

	for n in range(1, 5):
		ml = machineLearning()
		ml.setup(benign_directory='./benign_samples',
		malicious_directory='./malicious_samples',
		n_grams=n, show_progress_bar=False, feature_selection=True)

		print('--- n-gram: {} ---'.format(n))
	
		ml.decision_tree()
		ml.random_forest()
		ml.neural_network()
		ml.naive_bayes()
		ml.stochastic_gradient_descent()
		ml.knn()
		ml.svm()
	
	for n in range(1, 5):
		ml = machineLearning()
		ml.setup(benign_directory='./benign_samples',
		malicious_directory='./malicious_samples',
		n_grams=n, show_progress_bar=False, feature_selection=False)

		print('--- n-gram: {} ---'.format(n))
	
		ml.decision_tree()
		ml.random_forest()
		ml.neural_network()
		ml.naive_bayes()
		ml.stochastic_gradient_descent()
		ml.knn()
		ml.svm()
	'''
	ml.feature_vector_setup('/home/drc/Desktop/Research/grammar_malware_analysis/antlr/malicious_samples/1',
		'/home/drc/Desktop/Research/grammar_malware_analysis/antlr/malicious_samples/1',
		'rule_order.json')
	'''

	'''
	import argparse

	parser = argparse.ArgumentParser()
	parser.add_argument("parser", help="Path to parser to use", type=str)
	parser.add_argument("benign_directory", help="Directory containing the benign files", type=str)
	parser.add_argument("malicious_directory", help="Directory containing the malicious files", type=str)
	parser.add_argument("file_extensions", help="Comma delimited list of acceptable file extensions", type=str)
	args = parser.parse_args()

	extensions = []
	for ext in args.file_extensions.split(','):
		extensions.append('.{}'.format(ext))
	args.file_extensions = extensions

	ml = machineLearning(args.parser)
	
	ml.feature_vector_setup(args.benign_directory, args.malicious_directory, args.file_extensions);
	#print(ml.feature_set)
	ml.pickle_it()
	#ml.unpickle_it()
	'''

	'''
	ml.svm()
	ml.naive_bayes()
	ml.decision_tree()
	ml.random_forest()
	ml.neural_network()
	'''


if __name__ == '__main__':
    main()