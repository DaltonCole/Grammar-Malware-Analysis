import argparse
import os
import io
import sys
import progressbar
from threading import Thread, Lock

# NOTE ************************************************
# If number of times each grammar rule isn't useful, try percentage
# of how often the rule is used in the file (ie, normalize the data)
########################################################

class machineLearning:
	def __init__(self, parser_type):
		self.parser = parser_type()
		self.mutex = Lock()

	def feature_vector_setup(self, benign_directory: str, malicious_directory: str, file_extensions: list):
		"""Finds the feature dictionaries of each file in the benign and 
		malicious directories with the given file extensions

		Args:
			benign_directory (str): Directory containing the benign files
			malicious_directory (str): Directory containing the malicious files
			file_extensions ([str]): Acceptable file extensions

		Returns:
			Nothing. Sets two class instance variables
				self.benign_feature_dictionaries: [{"rule": count}]
				self.malicious_feature_dictionaries: [{"rule": count}]
				self.feature_set: [[]] List of list of features
				self.labels = [] Labels for each list of features in feature_set
				self.feature_order = [] Order in which features appear in feature_set
		"""
		# Get benign code file paths
		benign_file_paths = []
		for root, dirs, files in os.walk(benign_directory):
			for file in files:
				for file_extension in file_extensions:
					if file.endswith(file_extension):
						benign_file_paths.append(os.path.join(root, file))

		# Get malicious code file paths
		malicious_file_paths = []
		for root, dirs, files in os.walk(malicious_directory):
			for file in files:
				for file_extension in file_extensions:
					if file.endswith(file_extension):
						malicious_file_paths.append(os.path.join(root, file))

		# Feature labeled dictionaries
		self.benign_feature_dictionaries = []
		self.malicious_feature_dictionaries = []

		# Holds parsing threads
		threads = []
		# Progress bar
		count = -1
		bar = self._progress_bar(len(benign_file_paths), "Benign File Parsing")
		# For each file
		for file in benign_file_paths:
			# Multi-thread parsing
			threads.append(Thread(target = self._find_feature_dictionary, \
				args = (file, self.benign_feature_dictionaries)))
			# Start thread
			threads[-1].start()
			# For every 16 threads, wait, so we don't start too many threads
			if len(threads) >= 16:
				# Join threads, update progress bar
				for thread in threads:
					thread.join()
					count += 1
					bar.update(count + 1)
				threads = []

		# Join threads, update progress bar
		for thread in threads:
			thread.join()
			count += 1
			bar.update(count + 1)			
		bar.finish()

		# Reset parsing threads
		threads = []
		# Progress bar
		count = -1
		bar = self._progress_bar(len(benign_file_paths), "Malicious File Parsing")
		# For each file
		for file in benign_file_paths:
			# Multi-thread parsing
			threads.append(Thread(target = self._find_feature_dictionary, \
				args = (file, self.malicious_feature_dictionaries)))
			# Start thread
			threads[-1].start()
			# For every 16 threads, wait, so we don't start too many threads
			if len(threads) >= 16:
				# Join threads, update progress bar
				for thread in threads:
					thread.join()
					count += 1
					bar.update(count + 1)
				threads = []

		# Join threads, update progress bar
		for thread in threads:
			thread.join()
			count += 1
			bar.update(count + 1)			
		bar.finish()

		# Feature set and labels
		self.feature_set = []
		self.labels = []
		self.feature_order = []

		# Make sure we have elements from both classes
		if len(self.benign_feature_dictionaries) == 0:
			raise Exception("No benign elements")
		if len(self.malicious_feature_dictionaries) == 0:
			raise Exception("No malicious elements")

		# Get an ordering for features from unordered dict
		for key in self.benign_feature_dictionaries[0]:
			self.feature_order.append(key)

		for benign_feature_dict in self.benign_feature_dictionaries:
			features = []
			for feat in self.feature_order:
				features.append(benign_feature_dict[feat])
			self.feature_set.append(features)
			self.labels.append("Benign")

		for malicious_feature_dict in self.malicious_feature_dictionaries:
			features = []
			for feat in self.feature_order:
				features.append(malicious_feature_dict[feat])
			self.feature_set.append(features)
			self.labels.append("Malicious")

		sys.stderr = sys.stdout


	def _progress_bar(self, size: int, name: str):
		"""Returns a progress bar
		Args:
			size (int): Max size of progress bar (i.e. 20/20)
			name (str): What the progress bar is for
		"""
		print("Starting {}".format(name))
		bar = progressbar.ProgressBar(maxval = size, fd=sys.stdout, \
			widgets=[progressbar.Bar('=', '[', ']'), ' ', progressbar.Percentage()])
		bar.start()

		return bar

	def _find_feature_dictionary(self, file: str, feature_dictionaries: list):
		"""Find the feature dictionary of the specified file

		Supports multi-threading. Locks when adding features_dict to
		feature_dictionaries list.

		Uses self.mutex to lock resource.

		Args:
			file (str): File to parse and find the feature dictionary for
			feature_dictionaries ([{}]): List to add feature dictionary to

		Returns:
			Nothing. Appends features to feature_dictionaries list.
		"""
		# Get rules and rule usage
		rule_usage = self.parser.run_sample(file)

		self.mutex.acquire()
		try:
			if rule_usage != None:
				feature_dictionaries.append(rule_usage)
			else:
				pass
				#print("Failed file: {}".format(file))
		except:
			print("Append rule usage to feature_dictionaries FAILED")
		finally:
			self.mutex.release()


	def pickle_it(self):
		import pickle
		pickle.dump(self.feature_set, open("./pickle/feature_set.p", "wb"))
		pickle.dump(self.labels, open("./pickle/feature_labels.p", "wb"))
		pickle.dump(self.feature_order, open("./pickle/feature_order.p", "wb"))

	def unpickle_it(self):
		import pickle
		self.feature_set = pickle.load(open("./pickle/feature_set.p", "rb"))
		self.labels = pickle.load(open("./pickle/feature_labels.p", "rb"))
		self.feature_order = pickle.load(open("./pickle/feature_order.p", "rb"))

	def _cross_val_score(self, clf):
		from sklearn.model_selection import cross_val_score

		scores = cross_val_score(clf, self.feature_set, self.labels, cv=2)
		print("Accuracy: %0.2f (+/- %0.2f)" % (scores.mean(), scores.std() * 2))


	def svm(self):
		from sklearn import svm

		clf = svm.SVC()
		self._cross_val_score(clf)

	def naive_bayes(self):
		from sklearn.naive_bayes import GaussianNB

		gnd = GaussianNB()
		self._cross_val_score(gnd)

	def decision_tree(self):
		from sklearn import tree

		clf = tree.DecisionTreeClassifier()
		self._cross_val_score(clf)

	def random_forest(self):
		from sklearn.ensemble import RandomForestClassifier

		clf = RandomForestClassifier()
		self._cross_val_score(clf)

	def neural_network(self):
		from sklearn.neural_network import MLPClassifier

		# For small datasets, however, ‘lbfgs’ can converge faster and perform better.
		clf = MLPClassifier(solver='lbfgs', alpha=1e-5, \
							hidden_layer_sizes=(5,2), random_state=1)
		self._cross_val_score(clf)


def main():
	from Python3.antlr import Python3ParserHandler

	ml = machineLearning(Python3ParserHandler)
	#ml.feature_vector_setup(".bad/", "//bad/", [".py"])
	#ml.feature_vector_setup( "./samples/small_samples/", "./samples/small_samples/",[".py"])
	#ml.feature_vector_setup( "./samples/Python-Programs/", "./samples/python-scripts/",[".py"])
	#ml.pickle_it()
	ml.unpickle_it()

	ml.svm()
	ml.naive_bayes()
	ml.decision_tree()
	ml.random_forest()
	ml.neural_network()


if __name__ == '__main__':
    main()